{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b7d3acc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T16:43:11.087539Z",
     "iopub.status.busy": "2025-06-02T16:43:11.087171Z",
     "iopub.status.idle": "2025-06-02T16:43:11.093966Z",
     "shell.execute_reply": "2025-06-02T16:43:11.092817Z",
     "shell.execute_reply.started": "2025-06-02T16:43:11.087516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import Adam, AdamW, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daa5cf52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T16:41:13.281327Z",
     "iopub.status.busy": "2025-06-02T16:41:13.280971Z",
     "iopub.status.idle": "2025-06-02T16:41:14.739264Z",
     "shell.execute_reply": "2025-06-02T16:41:14.737890Z",
     "shell.execute_reply.started": "2025-06-02T16:41:13.281306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/VikramShenoy97/Human-Segmentation-Dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "570da81a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T17:55:07.825429Z",
     "iopub.status.busy": "2025-06-02T17:55:07.825074Z",
     "iopub.status.idle": "2025-06-02T17:55:07.833648Z",
     "shell.execute_reply": "2025-06-02T17:55:07.832548Z",
     "shell.execute_reply.started": "2025-06-02T17:55:07.825406Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((512,512)),\n",
    "            transforms.ToTesnsor()\n",
    "        ])\n",
    "\n",
    "        valid_extension = {\".jpg\",\".jpeg\",\".png\"}\n",
    "        self.images = [f for f in os.listdir(image_dir) if os.path.splittext(f)[1].lower() in valid_extension]\n",
    "\n",
    "    def __len(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem(self, idx):\n",
    "        image_path= os.path.join(self.image_dir, self.images[idx])\n",
    "        name, ext = os.path.splitext(self.images[idx])\n",
    "        mask_path= os.path.join(self.mask_dir, f\"{name}.png\")\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "        mask = (mask >0.5).float()\n",
    "\n",
    "        return image, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fee9f3b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:07:08.564821Z",
     "iopub.status.busy": "2025-06-02T18:07:08.563622Z",
     "iopub.status.idle": "2025-06-02T18:07:08.570140Z",
     "shell.execute_reply": "2025-06-02T18:07:08.568942Z",
     "shell.execute_reply.started": "2025-06-02T18:07:08.564778Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Data loader \n",
    "\n",
    "def get_dataloader(image_dir, maks_dir, batch_size =2, shuffle =True):\n",
    "    dataset = SegmentationDataset(image_dir, maks_dir)\n",
    "    return DataLoader(dataset, batch_size = batch_size, shuffle =shuffle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11cccdb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:44:03.525700Z",
     "iopub.status.busy": "2025-06-02T18:44:03.525318Z",
     "iopub.status.idle": "2025-06-02T18:44:03.532294Z",
     "shell.execute_reply": "2025-06-02T18:44:03.531257Z",
     "shell.execute_reply.started": "2025-06-02T18:44:03.525677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding =1),\n",
    "            nn.ReLU(inplace =True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding =1),\n",
    "            nn.ReLU(inplace =True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c340b793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:44:05.937747Z",
     "iopub.status.busy": "2025-06-02T18:44:05.937431Z",
     "iopub.status.idle": "2025-06-02T18:44:05.945266Z",
     "shell.execute_reply": "2025-06-02T18:44:05.943718Z",
     "shell.execute_reply.started": "2025-06-02T18:44:05.937727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## this downsample need to take care of as two conv and one max pooling\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool =nn.MaxPool2d(kernel_size =2, stride =2)\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x) ## convolve, this need be saved for upsampling\n",
    "        p = self.pool(down) ## the pooling stuff\n",
    "        return down, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c21bd7d8-6bb3-43c8-af52-6898547f72b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:44:07.849477Z",
     "iopub.status.busy": "2025-06-02T18:44:07.849103Z",
     "iopub.status.idle": "2025-06-02T18:44:07.857974Z",
     "shell.execute_reply": "2025-06-02T18:44:07.857225Z",
     "shell.execute_reply.started": "2025-06-02T18:44:07.849447Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        nn.ConvTranspose2d(in_channels, in_channels//2, kernel_size =2, stride =2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x1,x2],1)\n",
    "        return self.conv(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c59ce611",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T18:58:26.981767Z",
     "iopub.status.busy": "2025-06-02T18:58:26.981383Z",
     "iopub.status.idle": "2025-06-02T18:58:26.990601Z",
     "shell.execute_reply": "2025-06-02T18:58:26.989526Z",
     "shell.execute_reply.started": "2025-06-02T18:58:26.981744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Unet\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.down_conv1= DownSample(in_channels, 64) ## output is 64\n",
    "        self.down_conv2= DownSample(64, 128)\n",
    "        self.down_conv3= DownSample(128,256)\n",
    "        self.down_conv4= DownSample(256,512) \n",
    "\n",
    "        self.bottle_neck = DoubleConv(512, 1024)\n",
    "        \n",
    "        self.up_conv1= UpSample(1024, 512) ## output is 64\n",
    "        self.up_conv2= UpSample(512, 256)\n",
    "        self.up_conv3= UpSample(256,128)\n",
    "        self.up_conv4= UpSample(128,64)\n",
    "\n",
    "        self.out =nn.Conv2d(in_channels = 64, out_channels= num_classes, kernel_size =1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        down_1, p1 = self.down_conv1(x)\n",
    "        down_2, p2 = self.down_conv2(p1)\n",
    "        down_3, p3 = self.down_conv3(p2)\n",
    "        down_4, p4 = self.down_conv4(p3)\n",
    "\n",
    "        b = self.bottle_neck(p4)\n",
    "\n",
    "        up1= self.up_conv1(b,down_4)\n",
    "        up2= self.up_conv2(up1,down_3)\n",
    "        up3= self.up_conv3(up2,down_2)\n",
    "        up4= self.up_conv4(up3,down_1)\n",
    "\n",
    "        out =self.out(up4)\n",
    "        return out\n",
    "        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a833220-5e40-458c-ab36-c3d0dc05006e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T19:01:44.592711Z",
     "iopub.status.busy": "2025-06-02T19:01:44.591666Z",
     "iopub.status.idle": "2025-06-02T19:01:44.600690Z",
     "shell.execute_reply": "2025-06-02T19:01:44.599253Z",
     "shell.execute_reply.started": "2025-06-02T19:01:44.592678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## \n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth = 1e-16):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth =smooth\n",
    "    def forward(self,inputs, targets):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets =targets.view(-1)\n",
    "\n",
    "        intersection =(inputs * targets).sum()\n",
    "        dice_score = (2 * intersection + self.smooth)/(inputs.sum() + targets.sum() + self.smooth)\n",
    "\n",
    "        return 1 - dice_score\n",
    "\n",
    "\n",
    "\n",
    "class BCEWithDiceLoss(nn.Module): \n",
    "    def __init__(self, smooth =1e-6):\n",
    "        super(BCEWithDiceLoss, self).__init__()\n",
    "        self.bce =nn.BCEWithDiceLoss()\n",
    "        self.dice = DiceLoss()\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        bce_loss = self.bce(inputs, targets)\n",
    "        dice_loss =self.dice(inputs, targets)\n",
    "        return 0.5 * bce_loss + dice_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a4cc5d17-4bd8-4735-8438-dcb01ed37b1f",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "## Training loop  Loss\n",
    "\n",
    "def train(model, dataloader, epochs =2, lr =0.001, save_path  =\"unet_model\", load_path = None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    if load_path and os.path.exists(load_path):\n",
    "        print(f\"Loading model weights  form{load_path}\")\n",
    "        model.load_state_dict(torch.load(load_path, map_location =device))\n",
    "    else : \n",
    "        print(f\"no checkpoint found, training from scratch..\")\n",
    "\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = BCEWithDiceLoss()\n",
    "\n",
    "    optimizer = SGD(model.parameters(), lr =lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for images, masks, in dataloader:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            out = model(images)\n",
    "            \n",
    "            loss =criterion(output, masks)\n",
    "            loss.backward()\n",
    "            optimizer.steps()\n",
    "\n",
    "            epoch_loss +=loss.item()\n",
    "        \n",
    "        avg_loss =epoch_loss/ len(dataloader)\n",
    "\n",
    "        print(f\"epoch [{epoch+1}/{epochs}], Loss : {avg_loss: .4f}, LR: {lr}\")\n",
    "\n",
    "        if epoch %10==0 and epoch>0:\n",
    "            torch.save(model.state_dict(), f\"{save_path}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), f\"{save_path}_final.pth\")\n",
    "    print(f\"Model_ saved to {save_path}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71b38b8c-dc9c-46ee-99e1-e3008403c097",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchvision.transforms' has no attribute 'ToTesnsor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m Unet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(image_dir, maks_dir, batch_size, shuffle)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_dataloader\u001b[39m(image_dir, maks_dir, batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, shuffle \u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSegmentationDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaks_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(dataset, batch_size \u001b[38;5;241m=\u001b[39m batch_size, shuffle \u001b[38;5;241m=\u001b[39mshuffle)\n",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m, in \u001b[0;36mSegmentationDataset.__init__\u001b[1;34m(self, image_dir, mask_dir)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_dir \u001b[38;5;241m=\u001b[39m image_dir\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_dir \u001b[38;5;241m=\u001b[39m mask_dir\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m512\u001b[39m,\u001b[38;5;241m512\u001b[39m)),\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTesnsor\u001b[49m()\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m     10\u001b[0m valid_extension \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(image_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplittext(f)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01min\u001b[39;00m valid_extension]\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torchvision.transforms' has no attribute 'ToTesnsor'"
     ]
    }
   ],
   "source": [
    "dataloader = get_dataloader(\"\", \"\", batch_size =8, shuffle=True)\n",
    "\n",
    "model = Unet(in_channels=3, num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834cb76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, epochs =2, lr =0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e408e",
   "metadata": {},
   "source": [
    "### Inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bf65557",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load model and predict with stats\n",
    "def predict(model_path, input_image_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load model\n",
    "    model = UNet(in_channels=3, num_classes=1)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Track start time\n",
    "    total_start_time = time.time()\n",
    "\n",
    "    # Image preprocessing\n",
    "    preprocess_start_time = time.time()\n",
    "    image = Image.open(input_image_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "    preprocess_end_time = time.time()\n",
    "\n",
    "    # Model inference\n",
    "    inference_start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        output = torch.sigmoid(output)\n",
    "    inference_end_time = time.time()\n",
    "\n",
    "    # Postprocessing\n",
    "    postprocess_start_time = time.time()\n",
    "    mask = output.squeeze(0).squeeze(0).cpu().numpy()\n",
    "    mask = (mask > 0.4).astype(np.uint8) * 255\n",
    "    mask_image = Image.fromarray(mask)\n",
    "\n",
    "    combined = Image.new(\"RGB\", (512 * 2, 512))\n",
    "    combined.paste(image.resize((512, 512)), (0, 0))\n",
    "    combined.paste(mask_image.convert(\"RGB\"), (512, 0))\n",
    "    combined.save(\"output.jpg\")\n",
    "    postprocess_end_time = time.time()\n",
    "\n",
    "    # Calculate timing stats\n",
    "    total_end_time = time.time()\n",
    "\n",
    "    preprocess_time = preprocess_end_time - preprocess_start_time\n",
    "    inference_time = inference_end_time - inference_start_time\n",
    "    postprocess_time = postprocess_end_time - postprocess_start_time\n",
    "    total_time = total_end_time - total_start_time\n",
    "\n",
    "    # Print stats\n",
    "    print(\"\\nPrediction completed! Stats:\")\n",
    "    print(f\"  Image Preprocessing Time: {preprocess_time:.4f} seconds\")\n",
    "    print(f\"  Model Inference Time: {inference_time:.4f} seconds\")\n",
    "    print(f\"  Postprocessing Time: {postprocess_time:.4f} seconds\")\n",
    "    print(f\"  Total Prediction Time: {total_time:.4f} seconds\")\n",
    "    print(\"Prediction saved as output.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe10a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(model_path=\"/content/unet_model_80.pth\", input_image_path=\"/content/Human-Segmentation-Dataset/Training_Images/5.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc774c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213f6937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "CV-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
